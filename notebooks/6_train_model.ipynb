{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996198ec-a536-4c88-a7d9-c12ef22d32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "# Evaluate metrics on the test set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c4c915-e411-4a62-b2d9-9010c632877a",
   "metadata": {},
   "source": [
    "# **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24303fcb-d81b-4b1a-a286-94813d976a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(651, 4, 4, 4, 1539)\n",
      "(85, 4, 4, 4, 1539)\n",
      "(53, 4, 4, 4, 1539)\n",
      "(29, 4, 4, 4, 1539)\n",
      "(32499, 4, 4, 4, 1536)\n",
      "(4250, 4, 4, 4, 1536)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_actives = np.load('Path to npy file')\n",
    "test_actives = np.load('Path to npy file')\n",
    "train_decoys= np.load('Path to npy file')\n",
    "test_decoys = np.load('Path to npy file')\n",
    "test_actives_hard_test_1 = np.load('Path to npy file')\n",
    "test_actives_hard_test_2 = np.load('Path to npy file')\n",
    "print(train_actives.shape)\n",
    "print(test_actives.shape)\n",
    "print(test_actives_hard_test_1.shape)\n",
    "print(test_actives_hard_test_2.shape)\n",
    "print(train_decoys.shape)\n",
    "print(test_decoys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553732c5-a356-42b4-9d83-d155cc238cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(651, 4, 4, 4, 1539)\n",
      "(85, 4, 4, 4, 1539)\n",
      "(53, 4, 4, 4, 1539)\n",
      "(29, 4, 4, 4, 1539)\n",
      "(32499, 4, 4, 4, 1536)\n",
      "(4250, 4, 4, 4, 1536)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_actives = np.load('/home/juni/working/mettl3/voxels_data/grid_train_actives_splif.npy')\n",
    "test_actives = np.load('/home/juni/working/mettl3/voxels_data/grid_test_actives_splif.npy')\n",
    "train_decoys= np.load('/home/juni/working/mettl3/voxels_data/grid_train_decoys_splif_full.npy')\n",
    "test_decoys = np.load('/home/juni/working/mettl3/voxels_data/grid_test_decoys_splif_full.npy')\n",
    "test_actives_hard_test_1 = np.load('/home/juni/working/mettl3/voxels_data/grid_test_actives_splif_filtered_80.npy')\n",
    "test_actives_hard_test_2 = np.load('/home/juni/working/mettl3/voxels_data/grid_test_actives_splif_filtered_75.npy')\n",
    "print(train_actives.shape)\n",
    "print(test_actives.shape)\n",
    "print(test_actives_hard_test_1.shape)\n",
    "print(test_actives_hard_test_2.shape)\n",
    "print(train_decoys.shape)\n",
    "print(test_decoys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66d7f0a-9f65-49af-89d3-e15cc83ca2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actives = train_actives[:, :, :, :, :-3]\n",
    "test_actives = test_actives[:, :, :, :, :-3]\n",
    "test_actives_hard_test_1 = test_actives_hard_test_1[:, :, :, :, :-3]\n",
    "test_actives_hard_test_2 = test_actives_hard_test_2[:, :, :, :, :-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95bbbb-14dc-4710-8da7-f99b6abab2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90463fb-62d7-4e39-a97b-783fa53a60e4",
   "metadata": {},
   "source": [
    "# **Reshaping for CNN Input:**\n",
    "To feed this data into a 3D CNN in PyTorch, you typically need to structure it in the format (N, C, D, H, W), where:\n",
    "\n",
    "N is the batch size (number of samples).\\\n",
    "C is the number of channels/features (in this case, 1539).\\\n",
    "D, H, W are the depth, height, and width of the 3D grid (in this case, 4, 4, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a2f07d-f7b8-4dda-be87-4a6aed9d9c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([651, 1536, 4, 4, 4])\n",
      "torch.Size([85, 1536, 4, 4, 4])\n",
      "torch.Size([53, 1536, 4, 4, 4])\n",
      "torch.Size([29, 1536, 4, 4, 4])\n",
      "torch.Size([32499, 1536, 4, 4, 4])\n",
      "torch.Size([4250, 1536, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Reshape the data for PyTorch CNN input\n",
    "train_actives_tensor = torch.tensor(train_actives, dtype=torch.float32)\n",
    "test_actives_tensor = torch.tensor(test_actives, dtype=torch.float32)\n",
    "test_actives_hard_test_1_tensor = torch.tensor(test_actives_hard_test_1, dtype=torch.float32)\n",
    "test_actives_hard_test_2_tensor = torch.tensor(test_actives_hard_test_2, dtype=torch.float32)\n",
    "train_decoys_tensor = torch.tensor(train_decoys, dtype=torch.float32)\n",
    "test_decoys_tensor = torch.tensor(test_decoys, dtype=torch.float32)\n",
    "\n",
    "\n",
    "train_actives_tensor = train_actives_tensor.permute(0, 4, 1, 2, 3)  # Shape will be (651, 1539, 4, 4, 4)\n",
    "test_actives_tensor = test_actives_tensor.permute(0, 4, 1, 2, 3)  # Shape will be (651, 1539, 4, 4, 4)\n",
    "test_actives_hard_test_1_tensor = test_actives_hard_test_1_tensor.permute(0, 4, 1, 2, 3)  # Shape will be (651, 1539, 4, 4, 4)\n",
    "test_actives_hard_test_2_tensor = test_actives_hard_test_2_tensor.permute(0, 4, 1, 2, 3)\n",
    "train_decoys_tensor = train_decoys_tensor.permute(0, 4, 1, 2, 3)  # Shape will be (651, 1539, 4, 4, 4)\n",
    "test_decoys_tensor = test_decoys_tensor.permute(0, 4, 1, 2, 3)  # Shape will be (651, 1539, 4, 4, 4)\n",
    "\n",
    "print(train_actives_tensor.shape)\n",
    "print(test_actives_tensor.shape)\n",
    "print(test_actives_hard_test_1_tensor.shape)\n",
    "print(test_actives_hard_test_2_tensor.shape)\n",
    "print(train_decoys_tensor.shape)\n",
    "print(test_decoys_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407c7aab-1c65-4b92-85f0-2d123af67ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33150, 1536, 4, 4, 4])\n",
      "torch.Size([4335, 1536, 4, 4, 4])\n",
      "torch.Size([4303, 1536, 4, 4, 4])\n",
      "torch.Size([4279, 1536, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate training data\n",
    "train_data = torch.cat((train_actives_tensor, train_decoys_tensor), dim=0)  # Shape: (1302, 4, 4, 4, 1539)\n",
    "\n",
    "# Concatenate test data\n",
    "test_data = torch.cat((test_actives_tensor, test_decoys_tensor), dim=0)  # Shape: (170, 4, 4, 4, 1539)\n",
    "hard_test_data_1 = torch.cat((test_actives_hard_test_1_tensor, test_decoys_tensor), dim=0)  # Shape: (170, 4, 4, 4, 1539)\n",
    "hard_test_data_2 = torch.cat((test_actives_hard_test_2_tensor, test_decoys_tensor), dim=0)  # Shape: (170, 4, 4, 4, 1539)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(hard_test_data_1.shape)\n",
    "print(hard_test_data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf27684-e4e5-4637-8f34-6197acae22b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33150])\n",
      "torch.Size([4335])\n",
      "torch.Size([4303])\n",
      "torch.Size([4279])\n"
     ]
    }
   ],
   "source": [
    "# Create labels for training data\n",
    "train_labels = torch.cat((torch.ones(651), torch.zeros(32499)), dim=0)  # Shape: (1302,)\n",
    "\n",
    "# Create labels for test data\n",
    "test_labels = torch.cat((torch.ones(85), torch.zeros(4250)), dim=0)  # Shape: (170,)\n",
    "hard_test_1_labels = torch.cat((torch.ones(53), torch.zeros(4250)), dim=0)  # Shape: (170,)\n",
    "hard_test_2_labels = torch.cat((torch.ones(29), torch.zeros(4250)), dim=0)  # Shape: (170,)\n",
    "\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)\n",
    "print(hard_test_1_labels.shape)\n",
    "print(hard_test_2_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726054d1-a43c-4afe-8a77-13782829973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data and labels together\n",
    "train_indices = torch.randperm(train_data.size(0))\n",
    "train_data = train_data[train_indices]\n",
    "train_labels = train_labels[train_indices]\n",
    "\n",
    "# Shuffle test data and labels together\n",
    "test_indices = torch.randperm(test_data.size(0))\n",
    "test_data = test_data[test_indices]\n",
    "test_labels = test_labels[test_indices]\n",
    "\n",
    "# Shuffle hard test data 1 and labels together\n",
    "test_indices_filtered = torch.randperm(hard_test_data_1.size(0))\n",
    "test_data_filtered = hard_test_data_1[test_indices_filtered]\n",
    "test_labels_filtered = hard_test_1_labels[test_indices_filtered]\n",
    "\n",
    "# Shuffle hard test data 2 and labels together\n",
    "test_indices_filtered_1 = torch.randperm(hard_test_data_2.size(0))\n",
    "test_data_filtered_1 = hard_test_data_2[test_indices_filtered_1]\n",
    "test_labels_filtered_1 = hard_test_2_labels[test_indices_filtered_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc25cf0-cff9-4359-903c-ee3a4f55ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDataset for training and testing\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_dataset_filtered = TensorDataset(test_data_filtered, test_labels_filtered)\n",
    "test_dataset_filtered_1 = TensorDataset(test_data_filtered_1, test_labels_filtered_1)\n",
    "\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_filtered = DataLoader(test_dataset_filtered, batch_size=32, shuffle=False)\n",
    "test_loader_filtered_1 = DataLoader(test_dataset_filtered_1, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9950a6-1b81-45b3-89dc-bdcc4844421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652933d0-ab03-4461-8683-aa84114c2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def set_Seed(seed=41):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_Seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e26a85-b7e6-42de-af8c-e7d3d47cad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN3D(\n",
      "  (conv1): Conv3d(1536, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn_layers): ModuleList(\n",
      "    (0-5): 6 x MultiheadAttention3D(\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn2): MultiheadAttention3D(\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (conv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (bn3): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (attn3): MultiheadAttention3D(\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout_conv): Dropout3d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (dropout_fc1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (dropout_fc2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiheadAttention3D(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttention3D, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape (batch_size, channels, D, H, W) -> (batch_size, channels, D*H*W)\n",
    "        batch_size, channels, D, H, W = x.shape\n",
    "        x = x.view(batch_size, channels, -1).permute(2, 0, 1)  # Shape: (D*H*W, batch_size, channels)\n",
    "\n",
    "        # Apply Multihead Attention\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        attn_output = self.norm(attn_output)\n",
    "\n",
    "        # Reshape back to (batch_size, channels, D, H, W)\n",
    "        attn_output = attn_output.permute(1, 2, 0).view(batch_size, channels, D, H, W)\n",
    "        return attn_output\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3D, self).__init__()\n",
    "        # Input shape: (batch_size, 1539, 4, 4, 4)\n",
    "        self.conv1 = nn.Conv3d(1536, 32, kernel_size=3, stride=1, padding=1)  # Output: (batch_size, 512, 4, 4, 4)\n",
    "        self.bn1 = nn.BatchNorm3d(32)\n",
    "\n",
    "        #self.attn1 = MultiheadAttention3D(embed_dim=32, num_heads=1)  # Apply Multihead Attention\n",
    "        self.attn_layers = nn.ModuleList([MultiheadAttention3D(embed_dim=32, num_heads=8) for _ in range(6)])  # Apply Multihead Attention with 6 layers\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)  # Output: (batch_size, 256, 4, 4, 4)\n",
    "        self.bn2 = nn.BatchNorm3d(64)\n",
    "\n",
    "        self.attn2 = MultiheadAttention3D(embed_dim=64, num_heads=8)  # Apply Multihead Attention\n",
    "\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: (batch_size, 128, 4, 4, 4)\n",
    "        self.bn3 = nn.BatchNorm3d(128)\n",
    "\n",
    "        self.attn3 = MultiheadAttention3D(embed_dim=128, num_heads=8)  # Apply Multihead Attention\n",
    "\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)  # Output: (batch_size, 128, 2, 2, 2)\n",
    "        self.dropout_conv = nn.Dropout3d(p=0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2 * 2, 256)\n",
    "        self.dropout_fc1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout_fc2 = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(128, 1)  # Binary classification (e.g., active/inactive)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Conv1 + BatchNorm + ReLU\n",
    "        #Apply 6 layers of Multihead Attention sequentially\n",
    "        for attn_layer in self.attn_layers:\n",
    "             x = attn_layer(x)\n",
    "        #x = self.attn1(x)  # Multihead Attention after conv1\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Conv2 + BatchNorm + ReLU\n",
    "        #x = self.attn2(x)  # Multihead Attention after conv2\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Conv3 + BatchNorm + ReLU\n",
    "        #x = self.attn3(x)  # Multihead Attention after conv3\n",
    "\n",
    "        x = self.dropout_conv(x)  # Dropout after convolutions\n",
    "        x = self.pool(x)  # Pooling layer\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))  # Fully connected layer 1\n",
    "        x = self.dropout_fc1(x)  # Dropout after fc1\n",
    "        x = F.relu(self.fc2(x))  # Fully connected layer 2\n",
    "        x = self.dropout_fc2(x)  # Dropout after fc2\n",
    "        x = torch.sigmoid(self.fc3(x))  # Sigmoid for binary classification\n",
    "        #x = self.fc3(x)  # No activation function for regression\n",
    "        return x\n",
    "# Example usage:\n",
    "model = CNN3D()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b8858a8-c14c-4d6a-8dc1-aea8b7884a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.0215, Train Acc: 0.9937, Val Loss: 0.0027, Val Acc: 0.9991, Val F1: 0.9790, Val ROC-AUC: 0.9860\n",
      "Epoch 2/100, Train Loss: 0.0057, Train Acc: 0.9987, Val Loss: 0.0045, Val Acc: 0.9989, Val F1: 0.9753, Val ROC-AUC: 0.9791\n",
      "Epoch 3/100, Train Loss: 0.0006, Train Acc: 0.9998, Val Loss: 0.0056, Val Acc: 0.9994, Val F1: 0.9861, Val ROC-AUC: 0.9929\n",
      "Epoch 4/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0062, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 5/100, Train Loss: 0.0064, Train Acc: 0.9987, Val Loss: 0.0054, Val Acc: 0.9985, Val F1: 0.9648, Val ROC-AUC: 0.9755\n",
      "Epoch 6/100, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0079, Val Acc: 0.9989, Val F1: 0.9759, Val ROC-AUC: 0.9927\n",
      "Epoch 7/100, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0155, Val Acc: 0.9991, Val F1: 0.9792, Val ROC-AUC: 0.9894\n",
      "Epoch 8/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0166, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 9/100, Train Loss: 0.0026, Train Acc: 0.9995, Val Loss: 0.0104, Val Acc: 0.9983, Val F1: 0.9611, Val ROC-AUC: 0.9720\n",
      "Epoch 10/100, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0083, Val Acc: 0.9992, Val F1: 0.9825, Val ROC-AUC: 0.9860\n",
      "Epoch 11/100, Train Loss: 0.0019, Train Acc: 0.9995, Val Loss: 0.0055, Val Acc: 0.9986, Val F1: 0.9684, Val ROC-AUC: 0.9789\n",
      "Epoch 12/100, Train Loss: 0.0006, Train Acc: 0.9997, Val Loss: 0.0056, Val Acc: 0.9989, Val F1: 0.9758, Val ROC-AUC: 0.9893\n",
      "Epoch 13/100, Train Loss: 0.0041, Train Acc: 0.9998, Val Loss: 0.0055, Val Acc: 0.9994, Val F1: 0.9859, Val ROC-AUC: 0.9861\n",
      "Epoch 14/100, Train Loss: 0.0003, Train Acc: 0.9999, Val Loss: 0.0062, Val Acc: 0.9994, Val F1: 0.9859, Val ROC-AUC: 0.9861\n",
      "Epoch 15/100, Train Loss: 0.0022, Train Acc: 0.9995, Val Loss: 0.0026, Val Acc: 0.9991, Val F1: 0.9789, Val ROC-AUC: 0.9826\n",
      "Epoch 16/100, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0173, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 17/100, Train Loss: 0.0015, Train Acc: 0.9996, Val Loss: 0.0134, Val Acc: 0.9994, Val F1: 0.9861, Val ROC-AUC: 0.9929\n",
      "Epoch 18/100, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0164, Val Acc: 0.9989, Val F1: 0.9754, Val ROC-AUC: 0.9825\n",
      "Epoch 19/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0170, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 20/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0178, Val Acc: 0.9989, Val F1: 0.9754, Val ROC-AUC: 0.9825\n",
      "Epoch 21/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0197, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 22/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0190, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 23/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0902, Val Acc: 0.9989, Val F1: 0.9759, Val ROC-AUC: 0.9927\n",
      "Epoch 24/100, Train Loss: 0.0038, Train Acc: 1.0000, Val Loss: 0.0901, Val Acc: 0.9989, Val F1: 0.9759, Val ROC-AUC: 0.9927\n",
      "Epoch 25/100, Train Loss: 0.0054, Train Acc: 0.9989, Val Loss: 0.0128, Val Acc: 0.9986, Val F1: 0.9695, Val ROC-AUC: 0.9959\n",
      "Epoch 26/100, Train Loss: 0.0024, Train Acc: 0.9996, Val Loss: 0.0118, Val Acc: 0.9992, Val F1: 0.9827, Val ROC-AUC: 0.9928\n",
      "Epoch 27/100, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0081, Val Acc: 0.9989, Val F1: 0.9754, Val ROC-AUC: 0.9825\n",
      "Epoch 28/100, Train Loss: 0.0006, Train Acc: 0.9998, Val Loss: 0.0061, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 29/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0331, Val Acc: 0.9983, Val F1: 0.9630, Val ROC-AUC: 0.9958\n",
      "Epoch 30/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0096, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 31/100, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0241, Val Acc: 0.9970, Val F1: 0.9346, Val ROC-AUC: 0.9951\n",
      "Epoch 32/100, Train Loss: 0.0033, Train Acc: 0.9996, Val Loss: 0.0076, Val Acc: 0.9988, Val F1: 0.9718, Val ROC-AUC: 0.9790\n",
      "Epoch 33/100, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0062, Val Acc: 0.9992, Val F1: 0.9825, Val ROC-AUC: 0.9860\n",
      "Epoch 34/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0063, Val Acc: 0.9991, Val F1: 0.9790, Val ROC-AUC: 0.9860\n",
      "Epoch 35/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0071, Val Acc: 0.9992, Val F1: 0.9825, Val ROC-AUC: 0.9860\n",
      "Epoch 36/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0079, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 37/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0077, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 38/100, Train Loss: 0.0026, Train Acc: 0.9996, Val Loss: 0.0212, Val Acc: 0.9988, Val F1: 0.9726, Val ROC-AUC: 0.9926\n",
      "Epoch 39/100, Train Loss: 0.0032, Train Acc: 0.9994, Val Loss: 0.0131, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 40/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0153, Val Acc: 0.9992, Val F1: 0.9825, Val ROC-AUC: 0.9860\n",
      "Epoch 41/100, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0144, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 42/100, Train Loss: 0.0002, Train Acc: 0.9999, Val Loss: 0.0185, Val Acc: 0.9991, Val F1: 0.9792, Val ROC-AUC: 0.9894\n",
      "Epoch 43/100, Train Loss: 0.0005, Train Acc: 0.9998, Val Loss: 0.0160, Val Acc: 0.9985, Val F1: 0.9648, Val ROC-AUC: 0.9755\n",
      "Epoch 44/100, Train Loss: 0.0013, Train Acc: 0.9998, Val Loss: 0.0104, Val Acc: 0.9989, Val F1: 0.9758, Val ROC-AUC: 0.9893\n",
      "Epoch 45/100, Train Loss: 0.0002, Train Acc: 0.9999, Val Loss: 0.0167, Val Acc: 0.9992, Val F1: 0.9827, Val ROC-AUC: 0.9928\n",
      "Epoch 46/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0186, Val Acc: 0.9991, Val F1: 0.9792, Val ROC-AUC: 0.9894\n",
      "Epoch 47/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0188, Val Acc: 0.9991, Val F1: 0.9792, Val ROC-AUC: 0.9894\n",
      "Epoch 48/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0145, Val Acc: 0.9991, Val F1: 0.9790, Val ROC-AUC: 0.9860\n",
      "Epoch 49/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0159, Val Acc: 0.9991, Val F1: 0.9790, Val ROC-AUC: 0.9860\n",
      "Epoch 50/100, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0051, Val Acc: 0.9989, Val F1: 0.9753, Val ROC-AUC: 0.9791\n",
      "Epoch 51/100, Train Loss: 0.0014, Train Acc: 0.9996, Val Loss: 0.0038, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 52/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0043, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 53/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0051, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 54/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0058, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 55/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0072, Val Acc: 0.9991, Val F1: 0.9792, Val ROC-AUC: 0.9894\n",
      "Epoch 56/100, Train Loss: 0.0020, Train Acc: 0.9997, Val Loss: 0.0080, Val Acc: 0.9988, Val F1: 0.9720, Val ROC-AUC: 0.9824\n",
      "Epoch 57/100, Train Loss: 0.0044, Train Acc: 0.9992, Val Loss: 0.0060, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 58/100, Train Loss: 0.0004, Train Acc: 0.9997, Val Loss: 0.0069, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 59/100, Train Loss: 0.0004, Train Acc: 0.9999, Val Loss: 0.0067, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 60/100, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0188, Val Acc: 0.9989, Val F1: 0.9759, Val ROC-AUC: 0.9927\n",
      "Epoch 61/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0113, Val Acc: 0.9989, Val F1: 0.9758, Val ROC-AUC: 0.9893\n",
      "Epoch 62/100, Train Loss: 0.0012, Train Acc: 0.9998, Val Loss: 0.0136, Val Acc: 0.9985, Val F1: 0.9650, Val ROC-AUC: 0.9789\n",
      "Epoch 63/100, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0084, Val Acc: 0.9979, Val F1: 0.9493, Val ROC-AUC: 0.9548\n",
      "Epoch 64/100, Train Loss: 0.0012, Train Acc: 0.9998, Val Loss: 0.0119, Val Acc: 0.9985, Val F1: 0.9648, Val ROC-AUC: 0.9755\n",
      "Epoch 65/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0091, Val Acc: 0.9986, Val F1: 0.9682, Val ROC-AUC: 0.9755\n",
      "Epoch 66/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0080, Val Acc: 0.9986, Val F1: 0.9682, Val ROC-AUC: 0.9755\n",
      "Epoch 67/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0094, Val Acc: 0.9986, Val F1: 0.9682, Val ROC-AUC: 0.9755\n",
      "Epoch 68/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0071, Val Acc: 0.9994, Val F1: 0.9860, Val ROC-AUC: 0.9895\n",
      "Epoch 69/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0096, Val Acc: 0.9989, Val F1: 0.9751, Val ROC-AUC: 0.9757\n",
      "Epoch 70/100, Train Loss: 0.0025, Train Acc: 0.9995, Val Loss: 0.0138, Val Acc: 0.9980, Val F1: 0.9562, Val ROC-AUC: 0.9922\n",
      "Epoch 71/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0058, Val Acc: 0.9992, Val F1: 0.9823, Val ROC-AUC: 0.9826\n",
      "Epoch 72/100, Train Loss: 0.0005, Train Acc: 0.9998, Val Loss: 0.0126, Val Acc: 0.9985, Val F1: 0.9645, Val ROC-AUC: 0.9721\n",
      "Epoch 73/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0201, Val Acc: 0.9988, Val F1: 0.9722, Val ROC-AUC: 0.9858\n",
      "Epoch 74/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0240, Val Acc: 0.9988, Val F1: 0.9722, Val ROC-AUC: 0.9858\n",
      "Epoch 75/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0226, Val Acc: 0.9986, Val F1: 0.9686, Val ROC-AUC: 0.9823\n",
      "Epoch 76/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0277, Val Acc: 0.9988, Val F1: 0.9726, Val ROC-AUC: 0.9926\n",
      "Epoch 77/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0203, Val Acc: 0.9991, Val F1: 0.9793, Val ROC-AUC: 0.9927\n",
      "Epoch 78/100, Train Loss: 0.0052, Train Acc: 0.9998, Val Loss: 0.0092, Val Acc: 0.9988, Val F1: 0.9720, Val ROC-AUC: 0.9824\n",
      "Epoch 79/100, Train Loss: 0.0015, Train Acc: 0.9995, Val Loss: 0.0048, Val Acc: 0.9988, Val F1: 0.9724, Val ROC-AUC: 0.9892\n",
      "Epoch 80/100, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0094, Val Acc: 0.9989, Val F1: 0.9758, Val ROC-AUC: 0.9893\n",
      "Epoch 81/100, Train Loss: 0.0008, Train Acc: 0.9998, Val Loss: 0.0128, Val Acc: 0.9991, Val F1: 0.9792, Val ROC-AUC: 0.9894\n",
      "Epoch 82/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0332, Val Acc: 0.9982, Val F1: 0.9592, Val ROC-AUC: 0.9889\n",
      "Epoch 83/100, Train Loss: 0.0003, Train Acc: 0.9999, Val Loss: 0.0104, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 84/100, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0034, Val Acc: 0.9992, Val F1: 0.9826, Val ROC-AUC: 0.9894\n",
      "Epoch 85/100, Train Loss: 0.0006, Train Acc: 0.9997, Val Loss: 0.0077, Val Acc: 0.9989, Val F1: 0.9754, Val ROC-AUC: 0.9825\n",
      "Epoch 86/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0077, Val Acc: 0.9989, Val F1: 0.9754, Val ROC-AUC: 0.9825\n",
      "Epoch 87/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0071, Val Acc: 0.9989, Val F1: 0.9754, Val ROC-AUC: 0.9825\n",
      "Epoch 88/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0136, Val Acc: 0.9986, Val F1: 0.9691, Val ROC-AUC: 0.9891\n",
      "Epoch 89/100, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0201, Val Acc: 0.9977, Val F1: 0.9458, Val ROC-AUC: 0.9547\n",
      "Epoch 90/100, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0067, Val Acc: 0.9988, Val F1: 0.9716, Val ROC-AUC: 0.9756\n",
      "Epoch 91/100, Train Loss: 0.0003, Train Acc: 0.9998, Val Loss: 0.0082, Val Acc: 0.9988, Val F1: 0.9716, Val ROC-AUC: 0.9756\n",
      "Epoch 92/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0087, Val Acc: 0.9988, Val F1: 0.9716, Val ROC-AUC: 0.9756\n",
      "Epoch 93/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0074, Val Acc: 0.9988, Val F1: 0.9720, Val ROC-AUC: 0.9824\n",
      "Epoch 94/100, Train Loss: 0.0038, Train Acc: 1.0000, Val Loss: 0.0378, Val Acc: 0.9988, Val F1: 0.9720, Val ROC-AUC: 0.9824\n",
      "Epoch 95/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0377, Val Acc: 0.9988, Val F1: 0.9720, Val ROC-AUC: 0.9824\n",
      "Epoch 96/100, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0117, Val Acc: 0.9988, Val F1: 0.9718, Val ROC-AUC: 0.9790\n",
      "Epoch 97/100, Train Loss: 0.0012, Train Acc: 0.9998, Val Loss: 0.0360, Val Acc: 0.9980, Val F1: 0.9547, Val ROC-AUC: 0.9752\n",
      "Epoch 98/100, Train Loss: 0.0059, Train Acc: 0.9997, Val Loss: 0.0094, Val Acc: 0.9989, Val F1: 0.9756, Val ROC-AUC: 0.9859\n",
      "Epoch 99/100, Train Loss: 0.0010, Train Acc: 0.9998, Val Loss: 0.0072, Val Acc: 0.9988, Val F1: 0.9720, Val ROC-AUC: 0.9824\n",
      "Epoch 100/100, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0084, Val Acc: 0.9989, Val F1: 0.9758, Val ROC-AUC: 0.9893\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `train_loader` is the original full dataset.\n",
    "# You can split it directly into training and validation datasets as below\n",
    "\n",
    "# Split the dataset into training and validation (80% for training and 20% for validation)\n",
    "train_size = int(0.8 * len(train_loader.dataset))  # 80% for training\n",
    "val_size = len(train_loader.dataset) - train_size  # 20% for validation\n",
    "\n",
    "# Use random_split to split the data\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_loader.dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader_split = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader_split = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CNN3D().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize dictionaries to store metrics for training and validation\n",
    "train_metrics = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"roc_auc\": [],\n",
    "    \"pr_auc\": [],\n",
    "    \"mcc\": []\n",
    "}\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"roc_auc\": [],\n",
    "    \"pr_auc\": [],\n",
    "    \"mcc\": []\n",
    "}\n",
    "\n",
    "# Training and validation loops\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    for inputs, targets in train_loader_split:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "        train_predictions.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(targets.cpu().numpy())\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader_split)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_split:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs.squeeze(), targets)\n",
    "            val_running_loss += val_loss.item()\n",
    "            predicted = (outputs.squeeze() >= 0.5).float()\n",
    "            val_predictions.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(targets.cpu().numpy())\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "\n",
    "    val_loss_epoch = val_running_loss / len(val_loader_split)\n",
    "    val_accuracy_epoch = val_correct / val_total\n",
    "    val_losses.append(val_loss_epoch)\n",
    "    val_accuracies.append(val_accuracy_epoch)\n",
    "\n",
    "    # Compute validation metrics\n",
    "    val_precision = precision_score(val_labels, val_predictions, zero_division=0)\n",
    "    val_recall = recall_score(val_labels, val_predictions, zero_division=0)\n",
    "    val_f1 = f1_score(val_labels, val_predictions, zero_division=0)\n",
    "    val_roc_auc = roc_auc_score(val_labels, val_predictions)\n",
    "    val_pr_auc = average_precision_score(val_labels, val_predictions)\n",
    "    val_mcc = matthews_corrcoef(val_labels, val_predictions)\n",
    "\n",
    "    # Store validation metrics for this epoch\n",
    "    val_metrics[\"accuracy\"].append(val_accuracy_epoch)\n",
    "    val_metrics[\"precision\"].append(val_precision)\n",
    "    val_metrics[\"recall\"].append(val_recall)\n",
    "    val_metrics[\"f1_score\"].append(val_f1)\n",
    "    val_metrics[\"roc_auc\"].append(val_roc_auc)\n",
    "    val_metrics[\"pr_auc\"].append(val_pr_auc)\n",
    "    val_metrics[\"mcc\"].append(val_mcc)\n",
    "\n",
    "    # Compute training metrics\n",
    "    train_precision = precision_score(train_labels, train_predictions, zero_division=0)\n",
    "    train_recall = recall_score(train_labels, train_predictions, zero_division=0)\n",
    "    train_f1 = f1_score(train_labels, train_predictions, zero_division=0)\n",
    "    train_roc_auc = roc_auc_score(train_labels, train_predictions)\n",
    "    train_pr_auc = average_precision_score(train_labels, train_predictions)\n",
    "    train_mcc = matthews_corrcoef(train_labels, train_predictions)\n",
    "\n",
    "    # Store training metrics for this epoch\n",
    "    train_metrics[\"accuracy\"].append(epoch_accuracy)\n",
    "    train_metrics[\"precision\"].append(train_precision)\n",
    "    train_metrics[\"recall\"].append(train_recall)\n",
    "    train_metrics[\"f1_score\"].append(train_f1)\n",
    "    train_metrics[\"roc_auc\"].append(train_roc_auc)\n",
    "    train_metrics[\"pr_auc\"].append(train_pr_auc)\n",
    "    train_metrics[\"mcc\"].append(train_mcc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, Val Loss: {val_loss_epoch:.4f}, Val Acc: {val_accuracy_epoch:.4f}, Val F1: {val_f1:.4f}, Val ROC-AUC: {val_roc_auc:.4f}')\n",
    "\n",
    "# After training, you can directly plot or store the results for each epoch, such as accuracy, loss, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4e95ff-43e8-46c9-a007-0149a1d17fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models/model_checkpoint_predict_100epochs.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory where you want to save the model\n",
    "save_dir = './models'  # You can change this to your desired directory path\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define the path where the model will be saved\n",
    "save_path = os.path.join(save_dir, 'model_checkpoint_predict_100epochs.pth')\n",
    "\n",
    "# Save the model state_dict and optimizer state_dict (if needed)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': running_loss,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a09d5560-4fac-4adc-a318-2e84e37929e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set\n",
      "Test Accuracy: 0.9995\n",
      "PR-AUC: 0.9768\n",
      "ROC-AUC: 0.9940\n",
      "Precision: 0.9882\n",
      "Recall: 0.9882\n",
      "F1 Score: 0.9882\n",
      "MCC: 0.9880\n",
      "Hard test 1\n",
      "PR-AUC: 0.9629\n",
      "ROC-AUC: 0.9904\n",
      "Precision: 0.9811\n",
      "Recall: 0.9811\n",
      "F1 Score: 0.9811\n",
      "MCC: 0.9809\n",
      "Hard test 2\n",
      "PR-AUC: 0.9325\n",
      "ROC-AUC: 0.9826\n",
      "Precision: 0.9655\n",
      "Recall: 0.9655\n",
      "F1 Score: 0.9655\n",
      "MCC: 0.9653\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"test set\")# Final model evaluation on a completely different test set\n",
    "# Final model evaluation on a completely different test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_output.extend(outputs.cpu().numpy().astype(float))\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(targets.cpu().numpy())\n",
    "        test_correct += (predicted == targets).sum().item()\n",
    "        test_total += targets.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Evaluate metrics on the test set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "roc_auc = roc_auc_score(test_labels, test_predictions)\n",
    "pr_auc = average_precision_score(test_labels, test_predictions)\n",
    "mcc = matthews_corrcoef(test_labels, test_predictions)\n",
    "\n",
    "print(f'PR-AUC: {pr_auc:.4f}')\n",
    "print(f'ROC-AUC: {roc_auc:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'MCC: {mcc:.4f}')\n",
    "\n",
    "test_results_drA = pd.DataFrame({\"Predicted_score\": test_output})\n",
    "test_results_drA['Predicted_Activity'] = test_predictions\n",
    "test_results_drA['Observed_Activity'] = test_labels\n",
    "test_results_drA.to_csv(\"./final_model_results/test_VS_run_5.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Hard test 1\")\n",
    "# Final model evaluation on a completely different test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_filtered:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_output.extend(outputs.cpu().numpy())\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Evaluate metrics on the test set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef,precision_recall_curve\n",
    "\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "roc_auc = roc_auc_score(test_labels, test_predictions)\n",
    "pr_auc = average_precision_score(test_labels, test_predictions)\n",
    "mcc = matthews_corrcoef(test_labels, test_predictions)\n",
    "\n",
    "print(f'PR-AUC: {pr_auc:.4f}')\n",
    "print(f'ROC-AUC: {roc_auc:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'MCC: {mcc:.4f}')\n",
    "\n",
    "hard_test_1_results_drA = pd.DataFrame({\"Predicted_score\": test_output})\n",
    "hard_test_1_results_drA['Predicted_Activity'] = test_predictions\n",
    "hard_test_1_results_drA['Observed_Activity'] = test_labels\n",
    "hard_test_1_results_drA.to_csv(\"./final_model_results/hard_test_1_VS_run_5.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Hard test 2\")\n",
    "# Final model evaluation on a completely different test set\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "test_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader_filtered_1:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_output.extend(outputs.cpu().numpy().astype(float))\n",
    "        #test_output.extend(outputs.cpu().numpy())\n",
    "        predicted = (outputs.squeeze() >= 0.5).float()\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "# Evaluate metrics on the test set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef\n",
    "\n",
    "precision = precision_score(test_labels, test_predictions)\n",
    "recall = recall_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions)\n",
    "roc_auc = roc_auc_score(test_labels, test_predictions)\n",
    "pr_auc = average_precision_score(test_labels, test_predictions)\n",
    "mcc = matthews_corrcoef(test_labels, test_predictions)\n",
    "\n",
    "print(f'PR-AUC: {pr_auc:.4f}')\n",
    "print(f'ROC-AUC: {roc_auc:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'MCC: {mcc:.4f}')\n",
    "\n",
    "hard_test_2_results_drA = pd.DataFrame({\"Predicted_score\": test_output})\n",
    "hard_test_2_results_drA['Predicted_Activity'] = test_predictions\n",
    "hard_test_2_results_drA['Observed_Activity'] = test_labels\n",
    "hard_test_2_results_drA.to_csv(\"./final_model_results/hard_test_2_VS_run_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc18900-316a-4530-8040-4f5dccd785e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_score</th>\n",
       "      <th>Predicted_Activity</th>\n",
       "      <th>Observed_Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.8371831e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3.4420188e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[4.286056e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2.1032638e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.5208157e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>[2.1690974e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>[6.223316e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>[1.9869651e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>[1.4444923e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>[1.5270052e-07]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predicted_score  Predicted_Activity  Observed_Activity\n",
       "0     [1.8371831e-07]                 0.0                0.0\n",
       "1     [3.4420188e-07]                 0.0                0.0\n",
       "2      [4.286056e-07]                 0.0                0.0\n",
       "3     [2.1032638e-07]                 0.0                0.0\n",
       "4     [1.5208157e-07]                 0.0                0.0\n",
       "...               ...                 ...                ...\n",
       "4298  [2.1690974e-07]                 0.0                0.0\n",
       "4299   [6.223316e-07]                 0.0                0.0\n",
       "4300  [1.9869651e-07]                 0.0                0.0\n",
       "4301  [1.4444923e-07]                 0.0                0.0\n",
       "4302  [1.5270052e-07]                 0.0                0.0\n",
       "\n",
       "[4303 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_test_1_results_drA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddff8340-8381-4e76-b0d1-b616b771d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to fold_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Data structure to save\n",
    "results = {\n",
    "    \"fold_train_losses\": train_losses,\n",
    "    \"fold_val_losses\": val_losses,\n",
    "    \"fold_train_accuracies\": train_accuracies,\n",
    "    \"fold_val_accuracies\": val_accuracies\n",
    "}\n",
    "\n",
    "# Save data to a JSON file\n",
    "with open(\"./final_model_results/fold_results_run_5.json\", \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Results saved to fold_results.json\")\n",
    "\n",
    "# To load the data later\n",
    "with open(\"./final_model_results/fold_results_run_5.json\", \"r\") as f:\n",
    "    loaded_results = json.load(f)\n",
    "\n",
    "# Accessing loaded data\n",
    "loaded_fold_train_losses = loaded_results[\"fold_train_losses\"]\n",
    "loaded_fold_val_losses = loaded_results[\"fold_val_losses\"]\n",
    "loaded_fold_train_accuracies = loaded_results[\"fold_train_accuracies\"]\n",
    "loaded_fold_val_accuracies = loaded_results[\"fold_val_accuracies\"]\n",
    "\n",
    "# # Verify loaded data\n",
    "# print(\"Loaded Training Losses:\", loaded_fold_train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "325d96aa-a755-4e9d-9bd4-e8d662cccb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "PR-AUC: 0.9877\n",
      "ROC-AUC: 0.9959\n",
      "Precision: 0.9954\n",
      "Recall: 0.9918\n",
      "F1 Score: 0.9935\n",
      "MCC: 0.9934\n",
      "Validation\n",
      "PR-AUC: 0.9206\n",
      "ROC-AUC: 0.9678\n",
      "Precision: 0.9824\n",
      "Recall: 0.9359\n",
      "F1 Score: 0.9584\n",
      "MCC: 0.9580\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Training')\n",
    "train_metrics_pd = pd.DataFrame(train_metrics)\n",
    "train_metrics_pd.to_csv(\"./final_model_results/train_metrics_run_5.csv\")\n",
    "train_metrics_mean = train_metrics_pd.applymap(lambda x: sum(x) / len(x) if isinstance(x,list) else x)\n",
    "overallmean = train_metrics_mean.mean()\n",
    "print(f'PR-AUC: {overallmean[5]:.4f}')\n",
    "print(f'ROC-AUC: {overallmean[4]:.4f}')\n",
    "print(f'Precision: {overallmean[1]:.4f}')\n",
    "print(f'Recall: {overallmean[2]:.4f}')\n",
    "print(f'F1 Score: {overallmean[3]:.4f}')\n",
    "print(f'MCC: {overallmean[6]:.4f}')\n",
    "\n",
    "print('Validation')\n",
    "val_metrics_pd = pd.DataFrame(val_metrics)\n",
    "val_metrics_pd.to_csv(\"./final_model_results/val_metrics_run_5.csv\")\n",
    "val_metrics_mean = val_metrics_pd.applymap(lambda x: sum(x) / len(x) if isinstance(x,list) else x)\n",
    "overallmean = val_metrics_mean.mean()\n",
    "print(f'PR-AUC: {overallmean[5]:.4f}')\n",
    "print(f'ROC-AUC: {overallmean[4]:.4f}')\n",
    "print(f'Precision: {overallmean[1]:.4f}')\n",
    "print(f'Recall: {overallmean[2]:.4f}')\n",
    "print(f'F1 Score: {overallmean[3]:.4f}')\n",
    "print(f'MCC: {overallmean[6]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76824d-35a2-46b2-999e-5c62e92d6367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
